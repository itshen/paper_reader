"""
MCP æœåŠ¡å™¨ - è®ºæ–‡æœç´¢ä¸é˜…è¯»

è¿è¡Œæ–¹å¼:
    python3.11 server.py

è®¿é—®åœ°å€:
    Web ç•Œé¢: http://localhost:8633
    MCP ç«¯ç‚¹: http://localhost:8633/mcp

Copyright (c) 2025 Miyang Tech (Zhuhai Hengqin) Co., Ltd.
MIT License
"""

import os
import sys
import json
from typing import Optional
import yaml

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from mcp.server.fastmcp import FastMCP

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from auth import AuthManager
from api_logger import APILogger
from paper_tools import ArxivSearch, PaperCache, PDFConverter


# ==================== é…ç½®åŠ è½½ ====================

def load_config() -> tuple:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "config.yaml")
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f), config_path
    return {}, config_path


config, config_path = load_config()

# æœåŠ¡å™¨é…ç½®
SERVER_NAME = config.get("server", {}).get("name", "Paper Reader MCP")
SERVER_HOST = config.get("server", {}).get("host", "0.0.0.0")
SERVER_PORT = config.get("server", {}).get("port", 8633)

# æ•°æ®ç›®å½•
DATA_DIR = config.get("storage", {}).get("data_dir", "./data")
DB_PATH = os.path.join(DATA_DIR, "auth.db")
LOG_DB_PATH = os.path.join(DATA_DIR, "api_logs.db")

# è®ºæ–‡ç¼“å­˜é…ç½®
PAPERS_DIR = os.path.join(DATA_DIR, "papers")
PAPERS_DB_PATH = os.path.join(PAPERS_DIR, "papers.db")
PAPERS_PDF_DIR = os.path.join(PAPERS_DIR, "pdf")
PAPERS_MD_DIR = os.path.join(PAPERS_DIR, "markdown")

# è®ºæ–‡ç¼“å­˜é™åˆ¶
paper_config = config.get("papers", {})
PAPERS_MAX_SIZE_MB = paper_config.get("max_size_mb", 1024)  # é»˜è®¤ 1GB
PAPERS_MAX_AGE_DAYS = paper_config.get("max_age_days", 90)  # é»˜è®¤ 3 ä¸ªæœˆ

# åˆ›å»ºè®¤è¯ç®¡ç†å™¨
auth_manager = AuthManager(DB_PATH, config)

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
api_logger = APILogger(LOG_DB_PATH, max_records=1000)

# åˆ›å»ºè®ºæ–‡å·¥å…·
arxiv_search = ArxivSearch()
paper_cache = PaperCache(
    db_path=PAPERS_DB_PATH,
    pdf_dir=PAPERS_PDF_DIR,
    markdown_dir=PAPERS_MD_DIR,
    max_size_bytes=PAPERS_MAX_SIZE_MB * 1024 * 1024,
    max_age_days=PAPERS_MAX_AGE_DAYS
)
pdf_converter = PDFConverter()

# å­˜å‚¨ MCP ä¼šè¯çš„ token
mcp_session_tokens: dict = {}


# ==================== MCP æœåŠ¡å™¨å®ä¾‹ ====================

mcp = FastMCP(SERVER_NAME, host=SERVER_HOST, port=SERVER_PORT)


# ==================== MCP Token éªŒè¯ ====================

def verify_mcp_token(token: str) -> bool:
    """éªŒè¯ MCP API Token"""
    if not token:
        return False
    return auth_manager.verify_api_token(token)


def get_current_session_token() -> str:
    """è·å–å½“å‰ MCP ä¼šè¯çš„ token"""
    if len(mcp_session_tokens) == 1:
        return list(mcp_session_tokens.values())[0]
    if mcp_session_tokens:
        return list(mcp_session_tokens.values())[-1]
    return ""


# ==================== è®ºæ–‡å·¥å…· ====================

@mcp.tool()
def search_papers(
    query: str,
    max_results: int = 10,
    sort_by: str = "smart",
    sort_order: str = "descending",
    category: Optional[str] = None
) -> str:
    """
    æœç´¢ arXiv è®ºæ–‡
    
    é€šè¿‡å…³é”®è¯æœç´¢å­¦æœ¯è®ºæ–‡ï¼Œè¿”å›æ ‡é¢˜ã€æ‘˜è¦ç­‰ä¿¡æ¯ã€‚
    
    Args:
        query: æœç´¢å…³é”®è¯ï¼Œå¦‚ "machine learning"ã€"transformer attention"
        max_results: æœ€å¤§è¿”å›æ•°é‡ï¼Œé»˜è®¤ 10ï¼Œæœ€å¤š 50
        sort_by: æ’åºæ–¹å¼ï¼Œå¯é€‰å€¼ï¼š
                 - "smart": æ™ºèƒ½æ’åºï¼ˆé»˜è®¤ï¼‰ï¼Œç»¼åˆç›¸å…³æ€§å’Œæ—¶é—´ï¼Œè¶Šç›¸å…³ä¸”è¶Šæ–°çš„æ’è¶Šå‰
                 - "relevance": ä»…æŒ‰ç›¸å…³æ€§æ’åº
                 - "submitted": ä»…æŒ‰æäº¤æ—¶é—´æ’åº
                 - "updated": ä»…æŒ‰æ›´æ–°æ—¶é—´æ’åº
        sort_order: æ’åºé¡ºåºï¼Œå¯é€‰å€¼ï¼š
                    - "descending": é™åºï¼Œæœ€æ–°/æœ€ç›¸å…³ä¼˜å…ˆï¼ˆé»˜è®¤ï¼‰
                    - "ascending": å‡åºï¼Œæœ€æ—©/æœ€ä¸ç›¸å…³ä¼˜å…ˆ
        category: åˆ†ç±»è¿‡æ»¤ï¼Œ**å»ºè®®æŒ‡å®šåˆ†ç±»ä»¥è·å¾—æ›´ç²¾å‡†çš„ç»“æœ**ã€‚å¸¸ç”¨åˆ†ç±»ï¼š
                  - cs.AI: äººå·¥æ™ºèƒ½
                  - cs.CL: è®¡ç®—è¯­è¨€å­¦/NLPï¼ˆæ¨èç”¨äº LLMã€æ–‡æœ¬å¤„ç†ï¼‰
                  - cs.CV: è®¡ç®—æœºè§†è§‰ï¼ˆæ¨èç”¨äºå›¾åƒã€è§†é¢‘ï¼‰
                  - cs.LG: æœºå™¨å­¦ä¹ ï¼ˆæ¨èç”¨äºé€šç”¨ ML ç®—æ³•ï¼‰
                  - cs.NE: ç¥ç»ç½‘ç»œ/è¿›åŒ–è®¡ç®—
                  - cs.IR: ä¿¡æ¯æ£€ç´¢ï¼ˆæ¨èç”¨äºæœç´¢ã€æ¨èç³»ç»Ÿï¼‰
                  - cs.RO: æœºå™¨äºº
                  - cs.SE: è½¯ä»¶å·¥ç¨‹
                  - stat.ML: ç»Ÿè®¡æœºå™¨å­¦ä¹ 
                  - eess.AS: éŸ³é¢‘ä¸è¯­éŸ³å¤„ç†
                  - eess.IV: å›¾åƒä¸è§†é¢‘å¤„ç†
                  ä¸æŒ‡å®šåˆ†ç±»ä¼šæœç´¢å…¨éƒ¨é¢†åŸŸï¼Œç»“æœå¯èƒ½ä¸å¤Ÿç²¾å‡†
    
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼ŒåŒ…å« arXiv IDã€æ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸã€åˆ†ç±»
    """
    token = get_current_session_token()
    if not verify_mcp_token(token):
        return "è®¤è¯å¤±è´¥ï¼šè¯·åœ¨ MCP å®¢æˆ·ç«¯é…ç½®æœ‰æ•ˆçš„ API Token"
    
    try:
        # é™åˆ¶æœ€å¤§ç»“æœæ•°
        max_results = min(max_results, 50)
        
        # æœç´¢è®ºæ–‡
        papers = arxiv_search.search(
            query,
            max_results=max_results,
            sort_by=sort_by,
            sort_order=sort_order,
            category=category
        )
        
        if not papers:
            return (
                f"æœªæ‰¾åˆ°ä¸ \"{query}\" ç›¸å…³çš„è®ºæ–‡\n\n"
                f"ğŸ’¡ å»ºè®®ï¼š\n"
                f"1. å°è¯•ä½¿ç”¨è‹±æ–‡å…³é”®è¯æœç´¢ï¼ˆarXiv è®ºæ–‡ä¸»è¦æ˜¯è‹±æ–‡ï¼‰\n"
                f"2. ä½¿ç”¨æ›´é€šç”¨æˆ–æ›´å…·ä½“çš„å…³é”®è¯\n"
                f"3. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®"
            )
        
        # æ ¼å¼åŒ–ç»“æœ
        sort_by_names = {
            "smart": "æ™ºèƒ½æ’åºï¼ˆç›¸å…³æ€§+æ—¶é—´ï¼‰",
            "relevance": "ç›¸å…³æ€§",
            "submitted": "æäº¤æ—¶é—´",
            "updated": "æ›´æ–°æ—¶é—´"
        }
        sort_order_names = {"descending": "é™åº", "ascending": "å‡åº"}
        sort_info = f"{sort_by_names.get(sort_by, sort_by)} ({sort_order_names.get(sort_order, sort_order)})"
        
        lines = [f"ğŸ“š æœç´¢ç»“æœï¼š\"{query}\"ï¼ˆå…± {len(papers)} ç¯‡ï¼‰"]
        lines.append(f"ğŸ”„ æ’åº: {sort_info}")
        if category:
            lines.append(f"ğŸ·ï¸ åˆ†ç±»è¿‡æ»¤: {category}")
        lines.append("")
        
        for i, paper in enumerate(papers, 1):
            # æˆªæ–­æ‘˜è¦
            abstract = paper.abstract
            if len(abstract) > 300:
                abstract = abstract[:300] + "..."
            
            # ä½œè€…ï¼ˆæœ€å¤šæ˜¾ç¤º 3 ä¸ªï¼‰
            authors = paper.authors[:3]
            if len(paper.authors) > 3:
                authors.append(f"ç­‰ {len(paper.authors)} äºº")
            authors_str = ", ".join(authors)
            
            lines.append(f"---\n")
            lines.append(f"**{i}. {paper.title}**\n")
            lines.append(f"ğŸ“Œ arXiv ID: {paper.arxiv_id}")
            lines.append(f"ğŸ‘¤ ä½œè€…: {authors_str}")
            lines.append(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {paper.published}")
            lines.append(f"ğŸ·ï¸ åˆ†ç±»: {', '.join(paper.categories[:3])}")
            lines.append(f"\nğŸ“ æ‘˜è¦:\n{abstract}\n")
        
        lines.append("---")
        lines.append("\nğŸ’¡ æç¤ºï¼š")
        lines.append("â€¢ ä½¿ç”¨ `get_paper_content(arXiv ID)` è·å–è®ºæ–‡å…¨æ–‡")
        lines.append("â€¢ arXiv è®ºæ–‡ä¸»è¦æ˜¯è‹±æ–‡ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡å…³é”®è¯æœç´¢æ•ˆæœæ›´å¥½")
        
        return "\n".join(lines)
        
    except Exception as e:
        return f"âŒ æœç´¢å¤±è´¥: {str(e)}"


@mcp.tool()
def get_paper_content(
    paper_id: str,
    page: int = 1,
    max_chars: int = 20000
) -> str:
    """
    è·å–è®ºæ–‡å…¨æ–‡ï¼ˆMarkdown æ ¼å¼ï¼Œæ”¯æŒåˆ†é¡µï¼‰
    
    é€šè¿‡ arXiv ID ä¸‹è½½è®ºæ–‡ PDF å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼è¿”å›ã€‚
    è®ºæ–‡ä¼šç¼“å­˜åœ¨æœ¬åœ°ï¼Œè¶…è¿‡ 1GB æˆ– 3 ä¸ªæœˆä¼šè‡ªåŠ¨æ¸…ç†ã€‚
    
    Args:
        paper_id: arXiv è®ºæ–‡ IDï¼Œå¦‚ "2301.12345"
        page: é¡µç ï¼Œä» 1 å¼€å§‹ï¼Œé»˜è®¤ç¬¬ 1 é¡µ
        max_chars: æ¯é¡µæœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤ 20000ï¼ŒèŒƒå›´ 1000-100000
                   ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ­¤å€¼ï¼š
                   - å¦‚æœåªéœ€è¦å¿«é€Ÿæµè§ˆï¼Œå¯ä»¥è®¾ç½®è¾ƒå°å€¼å¦‚ 5000
                   - å¦‚æœéœ€è¦å®Œæ•´é˜…è¯»ï¼Œå¯ä»¥è®¾ç½®è¾ƒå¤§å€¼å¦‚ 50000
                   - å¦‚æœ Token é¢„ç®—æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°å€¼åˆ†å¤šæ¬¡è·å–
    
    Returns:
        è®ºæ–‡çš„ Markdown å†…å®¹ï¼ˆåˆ†é¡µï¼‰ï¼ŒåŒ…å«ï¼š
        - è®ºæ–‡å…ƒä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸï¼‰
        - æ‘˜è¦ï¼ˆAbstractï¼‰
        - åˆ†é¡µä¿¡æ¯ï¼ˆå½“å‰é¡µ/æ€»é¡µæ•°ã€æ€»å­—ç¬¦æ•°ï¼‰
        - æ­£æ–‡å†…å®¹
        - ä¸‹ä¸€é¡µè·å–æç¤ºï¼ˆå¦‚æœ‰ï¼‰
    """
    token = get_current_session_token()
    if not verify_mcp_token(token):
        return "è®¤è¯å¤±è´¥ï¼šè¯·åœ¨ MCP å®¢æˆ·ç«¯é…ç½®æœ‰æ•ˆçš„ API Token"
    
    try:
        # é™åˆ¶ max_chars èŒƒå›´
        max_chars = max(1000, min(max_chars, 100000))
        page = max(1, page)
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cached = paper_cache.get(paper_id)
        content = None
        title = None
        abstract = None
        authors = None
        published = None
        source = "æœ¬åœ°ç¼“å­˜"
        
        if cached and cached.markdown_path and os.path.exists(cached.markdown_path):
            # ç›´æ¥è¯»å–ç¼“å­˜çš„ Markdown
            with open(cached.markdown_path, "r", encoding="utf-8") as f:
                content = f.read()
            title = cached.title
            abstract = cached.abstract if hasattr(cached, 'abstract') else None
            published = cached.published
            # ä»ç¼“å­˜è·å–ä½œè€…
            import json
            try:
                authors = json.loads(cached.authors) if cached.authors else []
            except:
                authors = []
        
        if not content:
            # 2. è·å–è®ºæ–‡ä¿¡æ¯
            paper_info = arxiv_search.get_paper(paper_id)
            if not paper_info:
                return f"âŒ æœªæ‰¾åˆ°è®ºæ–‡: {paper_id}"
            
            title = paper_info.title
            abstract = paper_info.abstract
            authors = paper_info.authors
            published = paper_info.published
            source = "æ–°ä¸‹è½½"
            
            # 3. ä¿å­˜è®ºæ–‡å…ƒæ•°æ®åˆ°ç¼“å­˜
            paper_cache.save(
                arxiv_id=paper_id,
                title=title,
                abstract=abstract,
                authors=authors,
                published=published
            )
            
            # 4. ä¸‹è½½ PDFï¼ˆè‡ªåŠ¨éªŒè¯å’Œé‡è¯•ï¼‰
            pdf_path = paper_cache.get_pdf_path(paper_id)
            
            print(f"[Paper] æ­£åœ¨ä¸‹è½½ PDF: {paper_id}")
            success = arxiv_search.download_pdf(paper_id, pdf_path)
            if not success:
                return (
                    f"âŒ ä¸‹è½½ PDF å¤±è´¥: {paper_id}\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n"
                    f"2. arXiv æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨\n"
                    f"3. è¯¥è®ºæ–‡ PDF æš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"ğŸ’¡ å»ºè®®ç¨åé‡è¯•"
                )
            
            # 5. è½¬æ¢ä¸º Markdown
            markdown_path = paper_cache.get_markdown_path(paper_id)
            
            print(f"[Paper] æ­£åœ¨è½¬æ¢ PDF ä¸º Markdown: {paper_id}")
            content = pdf_converter.convert(pdf_path, markdown_path)
            
            # 6. æ›´æ–°ç¼“å­˜è·¯å¾„
            paper_cache.update_paths(paper_id, pdf_path=pdf_path, markdown_path=markdown_path)
        
        # è®¡ç®—åˆ†é¡µ
        total_chars = len(content)
        total_pages = (total_chars + max_chars - 1) // max_chars  # å‘ä¸Šå–æ•´
        total_pages = max(1, total_pages)
        
        # ç¡®ä¿é¡µç æœ‰æ•ˆ
        if page > total_pages:
            page = total_pages
        
        # æˆªå–å½“å‰é¡µå†…å®¹
        start_idx = (page - 1) * max_chars
        end_idx = min(start_idx + max_chars, total_chars)
        page_content = content[start_idx:end_idx]
        
        # æ„å»ºè¿”å›å†…å®¹
        lines = []
        lines.append(f"ğŸ“„ **{title}**\n")
        lines.append(f"ğŸ“Œ arXiv ID: {paper_id}")
        if authors:
            authors_str = ', '.join(authors[:5])
            if len(authors) > 5:
                authors_str += f" ç­‰ {len(authors)} äºº"
            lines.append(f"ğŸ‘¤ ä½œè€…: {authors_str}")
        lines.append(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {published}")
        lines.append(f"ğŸ’¾ æ¥æº: {source}")
        lines.append("")
        
        # æ·»åŠ æ‘˜è¦
        if abstract:
            lines.append("## ğŸ“ æ‘˜è¦")
            lines.append("")
            lines.append(abstract)
            lines.append("")
        
        # åˆ†é¡µä¿¡æ¯
        lines.append("---")
        lines.append(f"ğŸ“Š **åˆ†é¡µä¿¡æ¯**: ç¬¬ {page}/{total_pages} é¡µ | æ€»å­—ç¬¦æ•°: {total_chars} | æ¯é¡µ: {max_chars} å­—ç¬¦")
        if total_pages > 1:
            if page < total_pages:
                lines.append(f"ğŸ’¡ ä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page + 1})` è·å–ä¸‹ä¸€é¡µ")
            if page > 1:
                lines.append(f"ğŸ’¡ ä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page - 1})` è·å–ä¸Šä¸€é¡µ")
        lines.append("---")
        lines.append("")
        
        # æ­£æ–‡å†…å®¹
        lines.append("## ğŸ“– æ­£æ–‡å†…å®¹")
        lines.append("")
        lines.append(page_content)
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œæç¤ºå†…å®¹è¢«æˆªæ–­
        if page < total_pages:
            lines.append("")
            lines.append("---")
            lines.append(f"âš ï¸ å†…å®¹å·²æˆªæ–­ï¼Œä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page + 1})` è·å–ä¸‹ä¸€é¡µ")
        
        return "\n".join(lines)
        
    except Exception as e:
        return f"âŒ è·å–è®ºæ–‡å¤±è´¥: {str(e)}"


# ==================== å·¥å…·è°ƒç”¨æ˜ å°„ï¼ˆWeb æµ‹è¯•ç”¨ï¼‰ ====================

TOOL_MAP = {
    "search_papers": lambda p: _search_papers_internal(
        p.get("query", ""),
        int(p.get("max_results", 10) or 10),
        p.get("sort_by", "relevance"),
        p.get("sort_order", "descending"),
        p.get("category", None)
    ),
    "get_paper_content": lambda p: _get_paper_content_internal(
        p.get("paper_id", ""),
        int(p.get("page", 1) or 1),
        int(p.get("max_chars", 20000) or 20000)
    ),
}


# ==================== è®ºæ–‡å·¥å…·å†…éƒ¨å‡½æ•°ï¼ˆWeb æµ‹è¯•ç”¨ï¼‰ ====================

def _search_papers_internal(
    query: str,
    max_results: int = 10,
    sort_by: str = "smart",
    sort_order: str = "descending",
    category: Optional[str] = None
) -> str:
    """æœç´¢è®ºæ–‡ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    try:
        max_results = min(max_results, 50)
        papers = arxiv_search.search(
            query,
            max_results=max_results,
            sort_by=sort_by,
            sort_order=sort_order,
            category=category
        )
        
        if not papers:
            return (
                f"æœªæ‰¾åˆ°ä¸ \"{query}\" ç›¸å…³çš„è®ºæ–‡\n\n"
                f"ğŸ’¡ å»ºè®®ï¼š\n"
                f"1. å°è¯•ä½¿ç”¨è‹±æ–‡å…³é”®è¯æœç´¢ï¼ˆarXiv è®ºæ–‡ä¸»è¦æ˜¯è‹±æ–‡ï¼‰\n"
                f"2. ä½¿ç”¨æ›´é€šç”¨æˆ–æ›´å…·ä½“çš„å…³é”®è¯\n"
                f"3. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®"
            )
        
        sort_by_names = {
            "smart": "æ™ºèƒ½æ’åºï¼ˆç›¸å…³æ€§+æ—¶é—´ï¼‰",
            "relevance": "ç›¸å…³æ€§",
            "submitted": "æäº¤æ—¶é—´",
            "updated": "æ›´æ–°æ—¶é—´"
        }
        sort_order_names = {"descending": "é™åº", "ascending": "å‡åº"}
        sort_info = f"{sort_by_names.get(sort_by, sort_by)} ({sort_order_names.get(sort_order, sort_order)})"
        
        lines = [f"ğŸ“š æœç´¢ç»“æœï¼š\"{query}\"ï¼ˆå…± {len(papers)} ç¯‡ï¼‰"]
        lines.append(f"ğŸ”„ æ’åº: {sort_info}")
        if category:
            lines.append(f"ğŸ·ï¸ åˆ†ç±»è¿‡æ»¤: {category}")
        lines.append("")
        
        for i, paper in enumerate(papers, 1):
            abstract = paper.abstract
            if len(abstract) > 300:
                abstract = abstract[:300] + "..."
            
            authors = paper.authors[:3]
            if len(paper.authors) > 3:
                authors.append(f"ç­‰ {len(paper.authors)} äºº")
            authors_str = ", ".join(authors)
            
            lines.append(f"---\n")
            lines.append(f"**{i}. {paper.title}**\n")
            lines.append(f"ğŸ“Œ arXiv ID: {paper.arxiv_id}")
            lines.append(f"ğŸ‘¤ ä½œè€…: {authors_str}")
            lines.append(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {paper.published}")
            lines.append(f"ğŸ·ï¸ åˆ†ç±»: {', '.join(paper.categories[:3])}")
            lines.append(f"\nğŸ“ æ‘˜è¦:\n{abstract}\n")
        
        lines.append("---")
        lines.append("\nğŸ’¡ æç¤ºï¼š")
        lines.append("â€¢ ä½¿ç”¨ `get_paper_content(arXiv ID)` è·å–è®ºæ–‡å…¨æ–‡")
        lines.append("â€¢ arXiv è®ºæ–‡ä¸»è¦æ˜¯è‹±æ–‡ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡å…³é”®è¯æœç´¢æ•ˆæœæ›´å¥½")
        
        return "\n".join(lines)
        
    except Exception as e:
        return f"âŒ æœç´¢å¤±è´¥: {str(e)}"


def _get_paper_content_internal(paper_id: str, page: int = 1, max_chars: int = 20000) -> str:
    """è·å–è®ºæ–‡å…¨æ–‡ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œæ”¯æŒåˆ†é¡µï¼‰"""
    try:
        # é™åˆ¶ max_chars èŒƒå›´
        max_chars = max(1000, min(max_chars, 100000))
        page = max(1, page)
        
        cached = paper_cache.get(paper_id)
        content = None
        title = None
        abstract = None
        authors = None
        published = None
        source = "æœ¬åœ°ç¼“å­˜"
        
        if cached and cached.markdown_path and os.path.exists(cached.markdown_path):
            with open(cached.markdown_path, "r", encoding="utf-8") as f:
                content = f.read()
            title = cached.title
            abstract = cached.abstract if hasattr(cached, 'abstract') else None
            published = cached.published
            import json
            try:
                authors = json.loads(cached.authors) if cached.authors else []
            except:
                authors = []
        
        if not content:
            paper_info = arxiv_search.get_paper(paper_id)
            if not paper_info:
                return f"âŒ æœªæ‰¾åˆ°è®ºæ–‡: {paper_id}"
            
            title = paper_info.title
            abstract = paper_info.abstract
            authors = paper_info.authors
            published = paper_info.published
            source = "æ–°ä¸‹è½½"
            
            paper_cache.save(
                arxiv_id=paper_id,
                title=title,
                abstract=abstract,
                authors=authors,
                published=published
            )
            
            pdf_path = paper_cache.get_pdf_path(paper_id)
            
            print(f"[Paper] æ­£åœ¨ä¸‹è½½ PDF: {paper_id}")
            success = arxiv_search.download_pdf(paper_id, pdf_path)
            if not success:
                return (
                    f"âŒ ä¸‹è½½ PDF å¤±è´¥: {paper_id}\n\n"
                    f"å¯èƒ½åŸå› ï¼š\n"
                    f"1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n"
                    f"2. arXiv æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨\n"
                    f"3. è¯¥è®ºæ–‡ PDF æš‚æ—¶æ— æ³•è®¿é—®\n\n"
                    f"ğŸ’¡ å»ºè®®ç¨åé‡è¯•"
                )
            
            markdown_path = paper_cache.get_markdown_path(paper_id)
            
            print(f"[Paper] æ­£åœ¨è½¬æ¢ PDF ä¸º Markdown: {paper_id}")
            content = pdf_converter.convert(pdf_path, markdown_path)
            
            paper_cache.update_paths(paper_id, pdf_path=pdf_path, markdown_path=markdown_path)
        
        # è®¡ç®—åˆ†é¡µ
        total_chars = len(content)
        total_pages = (total_chars + max_chars - 1) // max_chars
        total_pages = max(1, total_pages)
        
        if page > total_pages:
            page = total_pages
        
        start_idx = (page - 1) * max_chars
        end_idx = min(start_idx + max_chars, total_chars)
        page_content = content[start_idx:end_idx]
        
        # æ„å»ºè¿”å›å†…å®¹
        lines = []
        lines.append(f"ğŸ“„ **{title}**\n")
        lines.append(f"ğŸ“Œ arXiv ID: {paper_id}")
        if authors:
            authors_str = ', '.join(authors[:5])
            if len(authors) > 5:
                authors_str += f" ç­‰ {len(authors)} äºº"
            lines.append(f"ğŸ‘¤ ä½œè€…: {authors_str}")
        lines.append(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {published}")
        lines.append(f"ğŸ’¾ æ¥æº: {source}")
        lines.append("")
        
        if abstract:
            lines.append("## ğŸ“ æ‘˜è¦")
            lines.append("")
            lines.append(abstract)
            lines.append("")
        
        lines.append("---")
        lines.append(f"ğŸ“Š **åˆ†é¡µä¿¡æ¯**: ç¬¬ {page}/{total_pages} é¡µ | æ€»å­—ç¬¦æ•°: {total_chars} | æ¯é¡µ: {max_chars} å­—ç¬¦")
        if total_pages > 1:
            if page < total_pages:
                lines.append(f"ğŸ’¡ ä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page + 1})` è·å–ä¸‹ä¸€é¡µ")
            if page > 1:
                lines.append(f"ğŸ’¡ ä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page - 1})` è·å–ä¸Šä¸€é¡µ")
        lines.append("---")
        lines.append("")
        lines.append("## ğŸ“– æ­£æ–‡å†…å®¹")
        lines.append("")
        lines.append(page_content)
        
        if page < total_pages:
            lines.append("")
            lines.append("---")
            lines.append(f"âš ï¸ å†…å®¹å·²æˆªæ–­ï¼Œä½¿ç”¨ `get_paper_content(\"{paper_id}\", page={page + 1})` è·å–ä¸‹ä¸€é¡µ")
        
        return "\n".join(lines)
        
    except Exception as e:
        return f"âŒ è·å–è®ºæ–‡å¤±è´¥: {str(e)}"


# ==================== è¿è¡ŒæœåŠ¡å™¨ ====================

def run_server():
    """è¿è¡Œ Web + MCP æœåŠ¡å™¨"""
    from contextlib import asynccontextmanager
    from fastapi import FastAPI, Request, Depends
    from fastapi.staticfiles import StaticFiles
    from fastapi.templating import Jinja2Templates
    from fastapi.responses import JSONResponse, RedirectResponse
    from mcp.server.streamable_http_manager import StreamableHTTPSessionManager
    from pydantic import BaseModel
    import uvicorn
    
    # Pydantic æ¨¡å‹
    class LoginRequest(BaseModel):
        username: str
        password: str
    
    class ChangePasswordRequest(BaseModel):
        new_password: str
    
    class CreateTokenRequest(BaseModel):
        name: str
    
    # MCP Session Manager
    session_manager = StreamableHTTPSessionManager(
        app=mcp._mcp_server,
        json_response=False,
        stateless=False,
    )
    
    @asynccontextmanager
    async def lifespan(app):
        async with session_manager.run():
            yield
    
    # åˆ›å»º FastAPI åº”ç”¨
    app = FastAPI(
        title=SERVER_NAME,
        description="è®ºæ–‡æœç´¢ä¸é˜…è¯» MCP æœåŠ¡",
        version="1.0.0",
        docs_url=None,
        redoc_url=None,
        openapi_url=None,
        lifespan=lifespan
    )
    
    # API æ—¥å¿—ä¸­é—´ä»¶
    import time as time_module
    from starlette.responses import Response
    
    @app.middleware("http")
    async def logging_middleware(request: Request, call_next):
        path = request.url.path
        
        # è·³è¿‡é™æ€æ–‡ä»¶ã€é¡µé¢è¯·æ±‚å’Œæ—¥å¿— API
        if (path.startswith("/static") or 
            path == "/favicon.ico" or 
            path.startswith("/api/logs") or
            not (path.startswith("/api") or path == "/mcp")):
            return await call_next(request)
        
        start_time = time_module.time()
        
        # è·å–è¯·æ±‚ä¿¡æ¯
        client_ip = request.client.host if request.client else None
        user_agent = request.headers.get("user-agent", "")
        method = request.method
        log_type = "mcp" if path == "/mcp" else "api"
        
        # è·å–è¯·æ±‚ä½“
        request_body = None
        if method in ["POST", "PUT", "PATCH"] and log_type == "api":
            try:
                body = await request.body()
                if body:
                    request_body = json.loads(body.decode("utf-8"))
            except:
                pass
        
        # è°ƒç”¨ä¸‹ä¸€ä¸ªå¤„ç†å™¨
        response = await call_next(request)
        
        # è¯»å–å“åº”ä½“
        response_body = None
        if response.status_code == 200:
            body_bytes = b""
            async for chunk in response.body_iterator:
                body_bytes += chunk
            
            try:
                response_body = json.loads(body_bytes.decode("utf-8"))
            except:
                response_body = body_bytes.decode("utf-8")[:500] if body_bytes else None
            
            # é‡æ–°æ„å»ºå“åº”
            response = Response(
                content=body_bytes,
                status_code=response.status_code,
                headers=dict(response.headers),
                media_type=response.media_type
            )
        
        # è®¡ç®—è€—æ—¶
        duration_ms = int((time_module.time() - start_time) * 1000)
        
        # è®°å½•æ—¥å¿—
        api_logger.log(
            log_type=log_type,
            method=method,
            path=path,
            request_body=request_body,
            response_status=response.status_code,
            response_body=response_body,
            duration_ms=duration_ms,
            client_ip=client_ip,
            user_agent=user_agent[:200] if user_agent else None
        )
        
        return response
    
    # é™æ€æ–‡ä»¶
    static_dir = os.path.join(os.path.dirname(__file__), "static")
    if os.path.exists(static_dir):
        app.mount("/static", StaticFiles(directory=static_dir), name="static")
    
    # æ¨¡æ¿
    templates_dir = os.path.join(os.path.dirname(__file__), "templates")
    templates = Jinja2Templates(directory=templates_dir)
    
    # ==================== è¾…åŠ©å‡½æ•° ====================
    
    def check_auth(request: Request) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        session_id = request.cookies.get("session_id")
        if not session_id:
            return False
        return auth_manager.verify_session(session_id) is not None
    
    def get_admin_id(request: Request) -> Optional[int]:
        """è·å–å½“å‰ç™»å½•çš„ç®¡ç†å‘˜ ID"""
        session_id = request.cookies.get("session_id")
        if not session_id:
            return None
        return auth_manager.verify_session(session_id)
    
    # ==================== é¡µé¢è·¯ç”± ====================
    
    @app.get("/login")
    async def login_page(request: Request):
        """ç™»å½•é¡µé¢"""
        if check_auth(request):
            return RedirectResponse(url="/", status_code=302)
        return templates.TemplateResponse("login.html", {"request": request})
    
    @app.get("/")
    async def index(request: Request):
        """é¦–é¡µ"""
        if not check_auth(request):
            return RedirectResponse(url="/login", status_code=302)
        return templates.TemplateResponse("index.html", {"request": request})
    
    @app.get("/test")
    async def test_page(request: Request):
        """å·¥å…·æµ‹è¯•é¡µé¢"""
        if not check_auth(request):
            return RedirectResponse(url="/login", status_code=302)
        return templates.TemplateResponse("test.html", {"request": request})
    
    @app.get("/admin")
    async def admin_page(request: Request):
        """ç®¡ç†é¡µé¢"""
        if not check_auth(request):
            return RedirectResponse(url="/login", status_code=302)
        return templates.TemplateResponse("admin.html", {"request": request})
    
    @app.get("/logs")
    async def logs_page(request: Request):
        """æ—¥å¿—é¡µé¢"""
        if not check_auth(request):
            return RedirectResponse(url="/login", status_code=302)
        return templates.TemplateResponse("logs.html", {"request": request})
    
    # ==================== è®¤è¯ API ====================
    
    @app.post("/api/auth/login")
    async def login(data: LoginRequest, request: Request):
        """ç™»å½•"""
        admin_id = auth_manager.verify_admin(data.username, data.password)
        
        if not admin_id:
            return JSONResponse({"success": False, "error": "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"})
        
        session_id = auth_manager.create_session(admin_id)
        response = JSONResponse({"success": True, "message": "ç™»å½•æˆåŠŸ"})
        response.set_cookie(
            key="session_id",
            value=session_id,
            httponly=True,
            max_age=86400,
            samesite="lax"
        )
        return response
    
    @app.post("/api/auth/logout")
    async def logout(request: Request):
        """ç™»å‡º"""
        session_id = request.cookies.get("session_id")
        if session_id:
            auth_manager.delete_session(session_id)
        
        response = JSONResponse({"success": True, "message": "å·²ç™»å‡º"})
        response.delete_cookie("session_id")
        return response
    
    @app.post("/api/auth/change-password")
    async def change_password(data: ChangePasswordRequest, request: Request):
        """ä¿®æ”¹å¯†ç """
        admin_id = get_admin_id(request)
        if not admin_id:
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        if len(data.new_password) < 6:
            return JSONResponse({"success": False, "error": "å¯†ç é•¿åº¦è‡³å°‘ 6 ä½"})
        
        success = auth_manager.change_password(admin_id, data.new_password)
        if success:
            return JSONResponse({"success": True, "message": "å¯†ç ä¿®æ”¹æˆåŠŸ"})
        else:
            return JSONResponse({"success": False, "error": "ä¿®æ”¹å¤±è´¥"})
    
    # ==================== Token ç®¡ç† API ====================
    
    @app.get("/api/tokens")
    async def list_tokens(request: Request):
        """åˆ—å‡ºæ‰€æœ‰ Token"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        tokens = auth_manager.list_api_tokens()
        return JSONResponse({"success": True, "tokens": tokens})
    
    @app.post("/api/tokens")
    async def create_token(data: CreateTokenRequest, request: Request):
        """åˆ›å»º Token"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        if not data.name:
            return JSONResponse({"success": False, "error": "è¯·è¾“å…¥ Token åç§°"})
        
        token = auth_manager.create_api_token(data.name)
        return JSONResponse({"success": True, "token": token})
    
    @app.delete("/api/tokens/{token_id}")
    async def delete_token(token_id: int, request: Request):
        """åˆ é™¤ Token"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        success = auth_manager.delete_api_token(token_id)
        if success:
            return JSONResponse({"success": True, "message": "Token å·²åˆ é™¤"})
        else:
            return JSONResponse({"success": False, "error": "åˆ é™¤å¤±è´¥"})
    
    # ==================== æ—¥å¿— API ====================
    
    @app.get("/api/logs")
    async def get_logs(
        request: Request,
        limit: int = 50,
        offset: int = 0,
        type: str = None,
        method: str = None,
        path: str = None
    ):
        """è·å–æ—¥å¿—åˆ—è¡¨"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        logs = api_logger.get_logs(
            limit=limit,
            offset=offset,
            log_type=type,
            method=method,
            path_contains=path
        )
        stats = api_logger.get_stats()
        
        return JSONResponse({
            "success": True,
            "logs": logs,
            "stats": stats
        })
    
    @app.get("/api/logs/{log_id}")
    async def get_log_detail(log_id: int, request: Request):
        """è·å–æ—¥å¿—è¯¦æƒ…"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        log = api_logger.get_log(log_id)
        if not log:
            return JSONResponse({"success": False, "error": "æ—¥å¿—ä¸å­˜åœ¨"})
        
        return JSONResponse({"success": True, "log": log})
    
    @app.delete("/api/logs")
    async def clear_logs(request: Request):
        """æ¸…ç©ºæ—¥å¿—"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        api_logger.clear_logs()
        return JSONResponse({"success": True, "message": "æ—¥å¿—å·²æ¸…ç©º"})
    
    # ==================== å·¥å…·è°ƒç”¨ APIï¼ˆWeb æµ‹è¯•ç”¨ï¼‰ ====================
    
    @app.post("/api/call")
    async def api_call(request: Request):
        """è°ƒç”¨å·¥å…· API"""
        if not check_auth(request):
            return JSONResponse({"success": False, "error": "æœªç™»å½•"})
        
        try:
            data = await request.json()
            tool_name = data.get("tool")
            params = data.get("params", {})
            
            if tool_name not in TOOL_MAP:
                return JSONResponse({
                    "success": False,
                    "error": f"æœªçŸ¥å·¥å…·: {tool_name}"
                })
            
            result = TOOL_MAP[tool_name](params)
            
            return JSONResponse({
                "success": True,
                "result": result
            })
            
        except Exception as e:
            return JSONResponse({
                "success": False,
                "error": str(e)
            })
    
    # ==================== MCP è·¯ç”± ====================
    
    async def handle_mcp(request: Request):
        """å¤„ç† MCP è¯·æ±‚"""
        # ä» header è·å– token
        auth_header = request.headers.get("authorization", "")
        if auth_header.startswith("Bearer "):
            token = auth_header[7:]
            session_id = request.headers.get("mcp-session-id", "")
            client_id = session_id or (
                f"{request.client.host}:{request.client.port}" 
                if request.client else "default"
            )
            mcp_session_tokens[client_id] = token
            print(f"[MCP] ä¼šè¯ {client_id[:20]}... å·²è®¤è¯")
        
        await session_manager.handle_request(
            request.scope, request.receive, request._send
        )
    
    app.add_api_route("/mcp", handle_mcp, methods=["GET", "POST", "DELETE"])
    
    # å¯åŠ¨æœåŠ¡
    print(f"\n{'='*50}")
    print(f"  {SERVER_NAME} å·²å¯åŠ¨")
    print(f"{'='*50}")
    print(f"  Web ç•Œé¢: http://localhost:{SERVER_PORT}")
    print(f"  ç®¡ç†åå°: http://localhost:{SERVER_PORT}/admin")
    print(f"  MCP æ—¥å¿—: http://localhost:{SERVER_PORT}/logs")
    print(f"  MCP ç«¯ç‚¹: http://localhost:{SERVER_PORT}/mcp")
    print(f"{'='*50}\n")
    
    uvicorn.run(app, host=SERVER_HOST, port=SERVER_PORT)


if __name__ == "__main__":
    run_server()
